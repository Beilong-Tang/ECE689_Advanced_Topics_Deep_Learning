{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f35942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pacakges\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b22e698f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:4\n"
     ]
    }
   ],
   "source": [
    "# Set the random seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67923232",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100 # Batch size\n",
    "# Load the dataset\n",
    "train_set = datasets.MNIST('./data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                       ]))\n",
    "train_loader = torch.utils.data.DataLoader(train_set, BATCH_SIZE)\n",
    "\n",
    "test_set = datasets.MNIST('./data', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc9e2045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data[0].shape)\n",
    "    print(data[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e115b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassification(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim = 28*28, out_dim = 10):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, out_dim, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8884531f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7840"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = SimpleClassification()\n",
    "sum(param.numel() for param in test.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84058cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n",
      "E 0, 0.000/600: Loss: 2.3616983890533447\n",
      "evaluation acc: 0.09650001674890518\n",
      "E 0, 200.000/600: Loss: 2.1203904151916504\n",
      "evaluation acc: 0.4345000088214874\n",
      "E 0, 400.000/600: Loss: 1.9179693460464478\n",
      "evaluation acc: 0.642300009727478\n",
      "Epoch #2\n",
      "E 1, 0.000/600: Loss: 1.7856677770614624\n",
      "evaluation acc: 0.7069999575614929\n",
      "E 1, 200.000/600: Loss: 1.735583782196045\n",
      "evaluation acc: 0.7375999093055725\n",
      "E 1, 400.000/600: Loss: 1.519203782081604\n",
      "evaluation acc: 0.7594999670982361\n",
      "Epoch #3\n",
      "E 2, 0.000/600: Loss: 1.4406170845031738\n",
      "evaluation acc: 0.7713997960090637\n",
      "E 2, 200.000/600: Loss: 1.4831302165985107\n",
      "evaluation acc: 0.7834998369216919\n",
      "E 2, 400.000/600: Loss: 1.2670003175735474\n",
      "evaluation acc: 0.7970998287200928\n",
      "Epoch #4\n",
      "E 3, 0.000/600: Loss: 1.2196794748306274\n",
      "evaluation acc: 0.8043997287750244\n",
      "E 3, 200.000/600: Loss: 1.313604474067688\n",
      "evaluation acc: 0.80899977684021\n",
      "E 3, 400.000/600: Loss: 1.100113034248352\n",
      "evaluation acc: 0.8158999681472778\n",
      "Epoch #5\n",
      "E 4, 0.000/600: Loss: 1.0708110332489014\n",
      "evaluation acc: 0.8180999755859375\n",
      "E 4, 200.000/600: Loss: 1.1943978071212769\n",
      "evaluation acc: 0.8206000924110413\n",
      "E 4, 400.000/600: Loss: 0.983345627784729\n",
      "evaluation acc: 0.8261001110076904\n",
      "Epoch #6\n",
      "E 5, 0.000/600: Loss: 0.9650976657867432\n",
      "evaluation acc: 0.828400194644928\n",
      "E 5, 200.000/600: Loss: 1.106506586074829\n",
      "evaluation acc: 0.8304000496864319\n",
      "E 5, 400.000/600: Loss: 0.8973930478096008\n",
      "evaluation acc: 0.8337000012397766\n",
      "Epoch #7\n",
      "E 6, 0.000/600: Loss: 0.8865350484848022\n",
      "evaluation acc: 0.8356999754905701\n",
      "E 6, 200.000/600: Loss: 1.0390018224716187\n",
      "evaluation acc: 0.8363999724388123\n",
      "E 6, 400.000/600: Loss: 0.8314382433891296\n",
      "evaluation acc: 0.8400000333786011\n",
      "Epoch #8\n",
      "E 7, 0.000/600: Loss: 0.8259365558624268\n",
      "evaluation acc: 0.8417999148368835\n",
      "E 7, 200.000/600: Loss: 0.9853909015655518\n",
      "evaluation acc: 0.8414999842643738\n",
      "E 7, 400.000/600: Loss: 0.7791219353675842\n",
      "evaluation acc: 0.8448998928070068\n",
      "Epoch #9\n",
      "E 8, 0.000/600: Loss: 0.7777643799781799\n",
      "evaluation acc: 0.8468000888824463\n",
      "E 8, 200.000/600: Loss: 0.9416462182998657\n",
      "evaluation acc: 0.8467000126838684\n",
      "E 8, 400.000/600: Loss: 0.7365090250968933\n",
      "evaluation acc: 0.8496001362800598\n",
      "Epoch #10\n",
      "E 9, 0.000/600: Loss: 0.7385163307189941\n",
      "evaluation acc: 0.8494001626968384\n",
      "E 9, 200.000/600: Loss: 0.905159592628479\n",
      "evaluation acc: 0.8507001399993896\n",
      "E 9, 400.000/600: Loss: 0.7010493278503418\n",
      "evaluation acc: 0.8526999950408936\n"
     ]
    }
   ],
   "source": [
    "model = SimpleClassification()\n",
    "\n",
    "AE_EPOCHS = 10 # Epochs for training the autoencoder\n",
    "# We use a binary cross-entropy loss for the reconstruction error\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "\n",
    "# Build the autoencoder\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(itertools.chain(model.parameters()),\n",
    "                             lr=1e-3)\n",
    "\n",
    "for i in range(AE_EPOCHS):\n",
    "    print('Epoch #{}'.format(i+1))\n",
    "\n",
    "    losses = []\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "\n",
    "        x, y = data\n",
    "        x = x.to(device) # [B, 1, H, W]\n",
    "        y = y.to(device)\n",
    "\n",
    "        x = x.squeeze(1).reshape(x.size(0), -1)\n",
    "\n",
    "        # Run the autoencoder\n",
    "        out = model(x)\n",
    "        loss = loss_f(out, y)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            print(f\"E {i}, {batch_idx:.3f}/{len(train_loader)}: Loss: {loss.item()}\")\n",
    "\n",
    "            ## Evaluate\n",
    "            total_acc = 0\n",
    "            for data in test_loader:\n",
    "                x, y = data\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                x = x.squeeze(1).reshape(x.size(0), -1)\n",
    "                out = model(x)\n",
    "                labels = torch.argmax(out, dim = 1) # [B]\n",
    "                acc = sum(labels == y) / labels.numel()\n",
    "                total_acc +=acc\n",
    "            print(f\"evaluation acc: {total_acc / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "781275d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class SimpleScratch():\n",
    "\n",
    "    def __init__(self, input_dim = 28*28, output_dim = 10):\n",
    "        self.W = torch.randn([input_dim, output_dim])\n",
    "        self.n_class = output_dim\n",
    "\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def _get_grad(self, x, y):\n",
    "        ## x: [B, M]\n",
    "        ## y: [B]\n",
    "        y_one_hot = F.one_hot(y, self.n_class).float()# [B, K]\n",
    "\n",
    "        out = self.predict(x) # [B, K]\n",
    "        soft = F.softmax(out, dim = -1) # [B,K]\n",
    "\n",
    "        grad_out = -y_one_hot + soft # [B, K]\n",
    "        grad_W = torch.bmm(x.unsqueeze(-1), grad_out.unsqueeze(1)) #[B,M,K]\n",
    "        grad_W = grad_W.mean(dim =0)\n",
    "        return grad_W\n",
    "    \n",
    "    def train(self, x, y, lr=0.001):\n",
    "        grad_W = self._get_grad(x, y)\n",
    "        self.W = self.W - lr * grad_W\n",
    "\n",
    "\n",
    "    def predict(self, x:torch.Tensor):\n",
    "        ## x: [B, M]\n",
    "        W = self.W.unsqueeze(0).repeat(x.size(0),1, 1) # [B, M, K]\n",
    "        out = torch.bmm(W.transpose(1,2), x.unsqueeze(-1)) # [B, K, 1]\n",
    "        return out.squeeze(-1) # [B, K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e3faf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([100, 784])\n",
      "y torch.Size([100])\n",
      "torch.Size([784, 10])\n"
     ]
    }
   ],
   "source": [
    "model = SimpleScratch()\n",
    "for data in train_loader:\n",
    "    x = data[0]\n",
    "    y = data[1]\n",
    "    break\n",
    "x = x.squeeze(1).reshape(x.size(0),-1)\n",
    "print(\"x\", x.shape)\n",
    "print(\"y\", y.shape)\n",
    "grad = model._get_grad(x, y)\n",
    "print(grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8205ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n",
      "E 0, 0.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.11789999902248383\n",
      "E 0, 200.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.20009993016719818\n",
      "E 0, 400.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.3082999587059021\n",
      "Epoch #2\n",
      "E 1, 0.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.38159993290901184\n",
      "E 1, 200.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.43469998240470886\n",
      "E 1, 400.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.48019999265670776\n",
      "Epoch #3\n",
      "E 2, 0.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.5152999758720398\n",
      "E 2, 200.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.544499933719635\n",
      "E 2, 400.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.5703999400138855\n",
      "Epoch #4\n",
      "E 3, 0.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.5906999111175537\n",
      "E 3, 200.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.6130999326705933\n",
      "E 3, 400.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.630099892616272\n",
      "Epoch #5\n",
      "E 4, 0.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.6470001935958862\n",
      "E 4, 200.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.6603001356124878\n",
      "E 4, 400.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.6716000437736511\n",
      "Epoch #6\n",
      "E 5, 0.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.6818998456001282\n",
      "E 5, 200.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.6912000179290771\n",
      "E 5, 400.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.6981000304222107\n",
      "Epoch #7\n",
      "E 6, 0.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.7074999809265137\n",
      "E 6, 200.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.7145999073982239\n",
      "E 6, 400.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.7207000255584717\n",
      "Epoch #8\n",
      "E 7, 0.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.7273000478744507\n",
      "E 7, 200.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.7336000204086304\n",
      "E 7, 400.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.7379000782966614\n",
      "Epoch #9\n",
      "E 8, 0.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.7426000237464905\n",
      "E 8, 200.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.7481999397277832\n",
      "E 8, 400.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.7512999773025513\n",
      "Epoch #10\n",
      "E 9, 0.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.7540000081062317\n",
      "E 9, 200.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.758400022983551\n",
      "E 9, 400.000/600: Loss: 0.8701011538505554\n",
      "evaluation acc: 0.7612998485565186\n"
     ]
    }
   ],
   "source": [
    "model = SimpleScratch()\n",
    "\n",
    "LR = 1.0e-2\n",
    "AE_EPOCHS = 10 # Epochs for training the autoencoder\n",
    "# We use a binary cross-entropy loss for the reconstruction error\n",
    "for i in range(AE_EPOCHS):\n",
    "    print('Epoch #{}'.format(i+1))\n",
    "\n",
    "    losses = []\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "\n",
    "        x, y = data\n",
    "\n",
    "        x = x.squeeze(1).reshape(x.size(0), -1)\n",
    "\n",
    "        # Run the autoencoder\n",
    "        model.train(x, y, lr = LR)\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            print(f\"E {i}, {batch_idx:.3f}/{len(train_loader)}: Loss: {loss.item()}\")\n",
    "\n",
    "            ## Evaluate\n",
    "            total_acc = 0\n",
    "            for data in test_loader:\n",
    "                x, y = data\n",
    "                x = x.squeeze(1).reshape(x.size(0), -1)\n",
    "                out = model.predict(x)\n",
    "                labels = torch.argmax(out, dim = 1) # [B]\n",
    "                acc = sum(labels == y) / labels.numel()\n",
    "                total_acc +=acc\n",
    "            print(f\"evaluation acc: {total_acc / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c06188c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[20., 20., 20.],\n",
      "         [ 1.,  1.,  1.]],\n",
      "\n",
      "        [[20., 20., 20.],\n",
      "         [ 1.,  1.,  1.]],\n",
      "\n",
      "        [[20., 20., 20.],\n",
      "         [ 1.,  1.,  1.]],\n",
      "\n",
      "        [[20., 20., 20.],\n",
      "         [ 1.,  1.,  1.]],\n",
      "\n",
      "        [[20., 20., 20.],\n",
      "         [ 1.,  1.,  1.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, 2, 3)\n",
    "x[0][0] = 20\n",
    "out = x.repeat(5, 1, 1)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
