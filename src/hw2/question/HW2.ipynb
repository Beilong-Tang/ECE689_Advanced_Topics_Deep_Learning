{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y_fk30kCvW9"
      },
      "source": [
        "# ECE 590, Spring 2024\n",
        "## Homework 2\n",
        "\n",
        "## Full name:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nplAHkFXCzeg"
      },
      "source": [
        "Feel free to submit in any format, but with content and code.\n",
        "- Example 1: pdf of report and folder of codes\n",
        "- Example 2: this jupyter-notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgEHJGopC3nP"
      },
      "source": [
        "## Question 1: Training a Light Score-Based Generative Model with Sliced Score Matching on MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G61Rlp1EDN3T"
      },
      "source": [
        "**Objective:** Implement and train a lightweight score-based generative model using the sliced score matching technique. The goal is to learn the data distribution's score for generating new samples similar to the training data.\n",
        "\n",
        "**Dataset:** Use the MNIST dataset, which consists of 70,000 28x28 grayscale images of handwritten digits (0-9). It is divided into 60,000 training images and 10,000 test images. MNIST can be found at https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html To reduce computational complexity, you can downscale the MNIST images to 7x7. Both score models trained with 28x28 and with 7x7 MNIST will get full credits.\n",
        "\n",
        "**Tasks:**\n",
        "1. Data preparation: Normalize the MNIST images to have pixel values between -1 and 1.\n",
        "2. Model Architecture: Construct a simple convolutional neural network (CNN) for estimating the data distribution's score. This network should accept a noisy image as input and output a score estimate.\n",
        "3. Sliced Score Matching: Implement the sliced score matching objective. Add Gaussian noise to the input images, and train the model to approximate the score of the noise-perturbed data distribution.\n",
        "4. Training: Use a smaller batch size if necessary to accommodate memory constraints. Train the model using a straightforward optimizer like Adam, with a conservative learning rate (e.g., 1e-3). Consider reducing the number of training epochs and implementing checkpointing to save the model intermittently.\n",
        "5. Evaluation and Generation: Evaluate the model qualitatively by visual inspection of generated digits.\n",
        "\n",
        "(A helpful website: https://github.com/mfkasim1/score-based-tutorial/blob/main/01-SGM-without-SDE.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgk6KUMID4uN"
      },
      "outputs": [],
      "source": [
        "\"\"\" Data Preparation \"\"\"\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define a transform to normalize the data and apply basic augmentations\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomAffine(degrees=5, translate=(0.05, 0.05)),  # slight rotation and translation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N741AJorEl7l"
      },
      "outputs": [],
      "source": [
        "\"\"\" Model Architecture \"\"\"\n",
        "# score_network takes input of MNIST image dimension and returns the output of\n",
        "# the same size, socre network usually follows the structure of U-Net\n",
        "class scoreNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(scoreNet, self).__init__()\n",
        "        \"\"\"YOUR CODE\"\"\"\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7tjII4rEopT"
      },
      "outputs": [],
      "source": [
        "\"\"\"Sliced Score Matching\"\"\"\n",
        "# Implement the sliced score matching function, as illustrated in sldes 26-28\n",
        "# of lectur 4 of Teaching Staff Lecture Slides on Sakai.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWJFtLB7Eqz9"
      },
      "outputs": [],
      "source": [
        "\"\"\"Training\"\"\"\n",
        "import time\n",
        "opt = torch.optim.Adam(score_network.parameters(), lr=3e-4)\n",
        "\n",
        "t0 = time.time()\n",
        "for i_epoch in range(5000):\n",
        "    total_loss = 0\n",
        "    for data, in train_loader:\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # training step\n",
        "        loss = calc_loss(scoreNet, data)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        # running stats\n",
        "        total_loss = total_loss + loss.detach().item() * data.shape[0]\n",
        "\n",
        "    # print the training stats\n",
        "    if i_epoch % 500 == 0:\n",
        "        print(f\"{i_epoch} ({time.time() - t0}s): {total_loss / len(dset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg1YVhSDEsay"
      },
      "outputs": [],
      "source": [
        "\"\"\"Evaluation and Generation\"\"\"\n",
        "# Sample 9 images from your trained score model and plot them in a 3x3 grid\n",
        "# using matplotlib's subplot function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtrgKMjtN0uA"
      },
      "source": [
        "## Question 2: Sampling from energy models with Langevin dynamics and stein scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAI5OQGsOAW1"
      },
      "source": [
        "Energy based models learn an energy functional $E_{\\theta}:\\mathcal{X}\\rightarrow \\mathbb{R}$. We look at the Gibbs distribution as follows:\n",
        "\n",
        "$p_{\\theta}(x) = \\frac{1}{Z_{\\theta}}e^{-E_{\\theta}(x)}$, where $Z_{\\theta} = \\int_{\\mathcal{X}}e^{-E_{\\theta}(y)}dy$.\n",
        "\n",
        "Directly sampling from $p_{\\theta}$ is hard, but we can approximate samples using a Markov chain with stationary distribution $p_{\\theta}$, spscifically, we have the discretized Langevin dynamics:\n",
        "\n",
        "<!-- '''$\\frac{d x_t}{dt} = \\nabla_x\\log p_{\\theta}(x_t)dt+\\sqrt{2}dW_t,$ -->\n",
        "\n",
        "<!-- where $dW_t$ is a white noise process, given by the Brownian motion $W_t$. -->\n",
        "\n",
        "<!-- (Diffusion following these dynamics converges asymptotically to samples $x_t\\sim p_{\\theta}$, in the sense that $D(x_t\\|p_{\\theta})→0$ as $t→∞$.) -->\n",
        "\n",
        "<!-- Discretizing the Langevin dynamics, we have -->\n",
        "\n",
        "$x_{t+1} = x_t-\\eta\\nabla_x\\log p_{\\theta}(x_t)+\\sqrt{2\\eta}\\epsilon_t,$\n",
        "\n",
        "where $\\epsilon_t\\sim\\mathcal{N}(0,I)$, $\\eta$ is the step size.\n",
        "\n",
        "We consider a 2D case, where $x\\in\\mathbb{R}^2$. Say $E_{\\theta}(x) = \\theta\\cdot x$, where $\\theta\\in\\mathbb{R}^{2}$ is a vector and has all the parameters.\n",
        "\n",
        "Calculate the expression for the distribution $x_N$, where $x_0\\sim \\mathcal{N}(0,I)$, and $N$ is the number of steps, in terms of $\\eta, \\theta, N$.\n",
        "\n",
        "(You can implement and see if your computational results match your analytical results. A helpful website: https://courses.cs.washington.edu/courses/cse599i/20au/resources/L16_ebm.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl5PDU_JVi-A"
      },
      "source": [
        "## Question 3: Transformer for translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wloM7bzeV1FV"
      },
      "source": [
        "Here, we implement transformers for neural machine translation (NMT), such as turning \"Hello world\" to \"Salut le monde\". You are going to follow the following steps:\n",
        "1. Load and prepare the data. We provide \"en-ft.txt\". Each line of this file contains an English phrase, the equivalent French phrase, and an attribution identifying where the translation came from. The en-fr.txt used in problem 3 can also be found at: https://github.com/jeffprosise/Applied-Machine-Learning/tree/main/Chapter%2013/Data\n",
        "2. Build and train a model. Implement a transformer from scratch in Pytorch. We will provide you with an existing implementation in Keras. You might also find https://github.com/gordicaleksa/pytorch-original-transformer useful.\n",
        "\n",
        "For deliverables, plot your training and validation accuracy. The x-axis should be epoch, the y-axis should be your translation accuracy.\n",
        "\n",
        "For reference, the provided code given at https://github.com/jeffprosise/Applied-Machine-Learning/blob/main/Chapter%2013/Neural%20Machine%20Translation%20(Transformer).ipynb achieves 85% accuracy after 14 epochs. You do not have to achieve the same performance to get full marks, just show understanding and functional codes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JI6PUrSdYKs"
      },
      "outputs": [],
      "source": [
        "\"\"\"Clean the text by removing punctuation symbols and numbers, converting\n",
        "characters to lowercase, and replacing Unicode characters with their ASCII\n",
        "equivalents. For the French samples, insert [start] and [end] tokens at the\n",
        " beginning and end of each phrase\"\"\"\n",
        "import pandas as pd\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "\n",
        "df = pd.read_csv('Data/en-fr.txt', names=['en', 'fr', 'attr'], usecols=['en', 'fr'], sep='\\t')\n",
        "df = df.sample(frac=1, random_state=42)\n",
        "df = df.reset_index(drop=True)\n",
        "df.head()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = normalize('NFD', text.lower())\n",
        "    text = re.sub('[^A-Za-z ]+', '', text)\n",
        "    return text\n",
        "\n",
        "def clean_and_prepare_text(text):\n",
        "    text = '[start] ' + clean_text(text) + ' [end]'\n",
        "    return text\n",
        "\n",
        "df['en'] = df['en'].apply(lambda row: clean_text(row))\n",
        "df['fr'] = df['fr'].apply(lambda row: clean_and_prepare_text(row))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbeUDWOVdftY"
      },
      "outputs": [],
      "source": [
        "\"\"\"The next step is to scan the phrases and determine the maximum length of the\n",
        "English phrases and then of the French phrases. These lengths will determine\n",
        "the lengths of the sequences input to and output from the model\"\"\"\n",
        "en = df['en']\n",
        "fr = df['fr']\n",
        "\n",
        "en_max_len = max(len(line.split()) for line in en)\n",
        "fr_max_len = max(len(line.split()) for line in fr)\n",
        "sequence_len = max(en_max_len, fr_max_len)\n",
        "\n",
        "print(f'Max phrase length (English): {en_max_len}')\n",
        "print(f'Max phrase length (French): {fr_max_len}')\n",
        "print(f'Sequence length: {sequence_len}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8nJpt-OdlRe"
      },
      "outputs": [],
      "source": [
        "\"\"\"Now fit one Tokenizer to the English phrases and another Tokenizer to their\n",
        "French equivalents, and generate padded sequences for all the phrases\"\"\"\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "en_tokenizer = Tokenizer()\n",
        "en_tokenizer.fit_on_texts(en)\n",
        "en_sequences = en_tokenizer.texts_to_sequences(en)\n",
        "en_x = pad_sequences(en_sequences, maxlen=sequence_len, padding='post')\n",
        "\n",
        "fr_tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n')\n",
        "fr_tokenizer.fit_on_texts(fr)\n",
        "fr_sequences = fr_tokenizer.texts_to_sequences(fr)\n",
        "fr_y = pad_sequences(fr_sequences, maxlen=sequence_len + 1, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKuUzYdMd0v2"
      },
      "outputs": [],
      "source": [
        "\"\"\"Compute the vocabulary sizes from the Tokenizer instances\"\"\"\n",
        "en_vocab_size = len(en_tokenizer.word_index) + 1\n",
        "fr_vocab_size = len(fr_tokenizer.word_index) + 1\n",
        "\n",
        "print(f'Vocabulary size (English): {en_vocab_size}')\n",
        "print(f'Vocabulary size (French): {fr_vocab_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5UEW_P_d4CF"
      },
      "outputs": [],
      "source": [
        "\"\"\"Finally, create the features and the labels the model will be trained with.\n",
        "The features are the padded English sequences and the padded French sequences\n",
        "minus the [end] tokens. The labels are the padded French sequences minus the\n",
        "[start] tokens. Package the features in a dictionary so they can be input to a\n",
        "model that accepts multiple inputs.\"\"\"\n",
        "inputs = { 'encoder_input': en_x, 'decoder_input': fr_y[:, :-1] }\n",
        "outputs = fr_y[:, 1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux5E3cxOefR_"
      },
      "source": [
        "Now, define and train the transformer in Pytorch. We provide here some example code in Keras, **but note that you have to write it in Pytorch**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcut4s6LeyFS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from keras_nlp.layers import TokenAndPositionEmbedding, TransformerEncoder\n",
        "from keras_nlp.layers import TransformerDecoder\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "sns.set()\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "num_heads = 8\n",
        "embed_dim = 256\n",
        "\n",
        "encoder_input = Input(shape=(None,), dtype='int64', name='encoder_input')\n",
        "x = TokenAndPositionEmbedding(en_vocab_size, sequence_len, embed_dim)(encoder_input)\n",
        "encoder_output = TransformerEncoder(embed_dim, num_heads)(x)\n",
        "encoded_seq_input = Input(shape=(None, embed_dim))\n",
        "\n",
        "decoder_input = Input(shape=(None,), dtype='int64', name='decoder_input')\n",
        "x = TokenAndPositionEmbedding(fr_vocab_size, sequence_len, embed_dim, mask_zero=True)(decoder_input)\n",
        "x = TransformerDecoder(embed_dim, num_heads)(x, encoded_seq_input)\n",
        "x = Dropout(0.4)(x)\n",
        "\n",
        "decoder_output = Dense(fr_vocab_size, activation='softmax')(x)\n",
        "decoder = Model([decoder_input, encoded_seq_input], decoder_output)\n",
        "decoder_output = decoder([decoder_input, encoder_output])\n",
        "\n",
        "model = Model([encoder_input, decoder_input], decoder_output)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary(line_length=120)\n",
        "\n",
        "callback = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "hist = model.fit(inputs, outputs, epochs=50, validation_split=0.2, callbacks=[callback])\n",
        "\n",
        "acc = hist.history['accuracy']\n",
        "val = hist.history['val_accuracy']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, '-', label='Training accuracy')\n",
        "plt.plot(epochs, val, ':', label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXBNo7ENgCIR"
      },
      "source": [
        "## Question 4: BERT for sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6DNvmkkgRR4"
      },
      "source": [
        "For the last problem, we are going to learn how to use the huggingface library to train a simple BERT classifier for sentiment analysis.\n",
        "\n",
        "We will use the IMDB dataset. You can find the dataset from huggingface using the following command:\n",
        "\n",
        "```\n",
        "from datasets import load_dataset\n",
        "imdb = load_dataset(\"imdb\")\n",
        "```\n",
        "To access BERT, use\n",
        "```\n",
        "from transformers import BertForSequenceClassification\n",
        "#load pre-trained BERT\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
        "                                                      num_labels = len(label_dict),\n",
        "                                                      output_attentions = False,\n",
        "                                                      output_hidden_states = False)\n",
        "```\n",
        "To reduce training complexity, you can choose to freeze the weight of the pretrained BERT model and only train the classifier. The classifier should have a minimum of 3 layers.\n",
        "You might find https://huggingface.co/blog/sentiment-analysis-python and https://github.com/baotramduong/Twitter-Sentiment-Analysis-with-Deep-Learning-using-BERT/blob/main/Notebook.ipynb helpful.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
